{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_samples = 100 # data sample count\n",
    "time_steps = 200  # Number of time steps in each sample\n",
    "num_sensors = 6   # Number of sensors\n",
    "\n",
    "# Define the full file paths for your files\n",
    "data_files = [\n",
    "    'sensor1_data.txt',\n",
    "    'sensor2_data.txt',\n",
    "    'sensor3_data.txt',\n",
    "    'sensor4_data.txt',\n",
    "    'sensor5_data.txt',\n",
    "    'sensor6_data.txt'\n",
    "]\n",
    "\n",
    "label_file = \"label_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store data for each channel\n",
    "input_data = []\n",
    "input_labels = []\n",
    "\n",
    "# Load data from each text file and append it to channel_data\n",
    "for filename in data_files:\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        channel_values = [list(map(float, line.strip().split())) for line in lines]\n",
    "        input_data.append(channel_values)\n",
    "\n",
    "x = np.array(input_data).transpose(1, 2, 0)\n",
    "\n",
    "# Load labels from the label file\n",
    "with open(label_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    input_labels = [int(value) for line in lines for value in line.strip().split()]\n",
    "\n",
    "y_labels = np.array(input_labels)\n",
    "\n",
    "y = to_categorical(y_labels-1,num_classes=2)  # One-hot encode the numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 200, 6)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# check the data in right shape\n",
    "print(x.shape)\n",
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(time_steps, num_sensors)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))  # 2 different people\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9472 - loss: 0.1512 - val_accuracy: 0.8000 - val_loss: 0.8713\n",
      "Epoch 2/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8942 - loss: 0.2405 - val_accuracy: 0.2500 - val_loss: 2.7443\n",
      "Epoch 3/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8871 - loss: 0.6967 - val_accuracy: 0.4500 - val_loss: 1.6225\n",
      "Epoch 4/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9382 - loss: 0.4168 - val_accuracy: 0.6500 - val_loss: 0.8332\n",
      "Epoch 5/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8751 - loss: 0.4362 - val_accuracy: 0.4000 - val_loss: 1.5280\n",
      "Epoch 6/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8594 - loss: 0.3127 - val_accuracy: 0.6000 - val_loss: 0.9838\n",
      "Epoch 7/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8396 - loss: 0.8464 - val_accuracy: 0.1500 - val_loss: 3.1985\n",
      "Epoch 8/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9352 - loss: 0.3700 - val_accuracy: 0.4500 - val_loss: 1.4530\n",
      "Epoch 9/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9390 - loss: 0.1572 - val_accuracy: 0.3000 - val_loss: 1.9532\n",
      "Epoch 10/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8646 - loss: 0.4919 - val_accuracy: 0.4500 - val_loss: 1.5534\n",
      "Epoch 11/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9265 - loss: 0.8452 - val_accuracy: 0.4500 - val_loss: 1.7075\n",
      "Epoch 12/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9221 - loss: 0.4058 - val_accuracy: 0.4000 - val_loss: 1.8013\n",
      "Epoch 13/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9222 - loss: 0.2216 - val_accuracy: 0.3500 - val_loss: 2.1402\n",
      "Epoch 14/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8956 - loss: 0.7315 - val_accuracy: 0.4000 - val_loss: 1.7143\n",
      "Epoch 15/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9314 - loss: 0.1239 - val_accuracy: 0.3500 - val_loss: 2.0570\n",
      "Epoch 16/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9772 - loss: 0.1006 - val_accuracy: 0.3000 - val_loss: 2.4904\n",
      "Epoch 17/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9418 - loss: 0.3704 - val_accuracy: 0.2000 - val_loss: 2.7830\n",
      "Epoch 18/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9783 - loss: 0.0824 - val_accuracy: 0.2500 - val_loss: 2.0512\n",
      "Epoch 19/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9442 - loss: 0.1193 - val_accuracy: 0.1500 - val_loss: 3.4962\n",
      "Epoch 20/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9977 - loss: 0.0318 - val_accuracy: 0.2500 - val_loss: 2.6047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17de21ac0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(x, y, epochs=20, batch_size=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('trained_model.h5') # Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "model = load_model('trained_model.h5') # Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full file paths for your files\n",
    "test_data_files = [\n",
    "    'sensor1_test.txt',\n",
    "    'sensor2_test.txt',\n",
    "    'sensor3_test.txt',\n",
    "    'sensor4_test.txt',\n",
    "    'sensor5_test.txt',\n",
    "    'sensor6_test.txt'\n",
    "]\n",
    "\n",
    "test_label_file = \"label_test.txt\"\n",
    "\n",
    "# Initialize an empty list to store data for each channel\n",
    "input_test_data = []\n",
    "input_test_labels = []\n",
    "\n",
    "# Load data from each text file and append it to channel_data\n",
    "for filename in test_data_files:\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        channel_values = [list(map(float, line.strip().split())) for line in lines]\n",
    "        input_test_data.append(channel_values)\n",
    "\n",
    "x_test = np.array(input_test_data).transpose(1, 2, 0)\n",
    "\n",
    "# Load labels from the label file\n",
    "with open(test_label_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    input_test_labels = [int(value) for line in lines for value in line.strip().split()]\n",
    "\n",
    "y_test_labels = np.array(input_test_labels)\n",
    "\n",
    "y_test = to_categorical(y_test_labels-1,num_classes=2)  # One-hot encode the numerical labels\n",
    "\n",
    "# print(x_test)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.6538 - loss: 1.1914\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0]\n",
      "Test Loss: 1.1914, Test Accuracy: 0.6538\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_binary = np.argmax(y_pred, axis=1)\n",
    "y_true_binary = np.argmax(y_test, axis=1)\n",
    "print(y_true_binary)\n",
    "print(y_pred_binary)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "a = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"\n",
    "b = \"2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\"\n",
    "\n",
    "l = b.strip().split()\n",
    "print(len(l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
